# checkpoint path, only enabled during inference
ckpt_path: 'path/to/your/checkpoint.pt'

# imagenet safetensor data, see datasets/img_latent_dataset.py for details
data:
  data_path: 'path/to/your/data'
  # fid reference file, see ADM<https://github.com/openai/guided-diffusion> for details
  fid_reference_file: 'path/to/your/VIRTUAL_imagenet256_labeled.npz'
  image_size: 256
  num_classes: 1000
  num_workers: 8
  # latent normalization, originated from our previous research FasterDiT <https://arxiv.org/abs/2410.10356>
  # The standard deviation of latents directly affects the SNR distribution during training
  # Channel-wise normalization provides stability but may not be optimal for all cases.
  latent_norm: true
  latent_multiplier: 1.0

# our pre-trained vision foundation model aligned VAE. see VA-VAE <https://arxiv.org/abs/2501.01423> for details. 
vae:
  model_name: 'vavae_f16d32'
  downsample_ratio: 16

# We explored several optimization techniques for transformers:
model:
  model_type: LightningDiT-XL/1
  use_qknorm: false
  use_swiglu: true
  use_rope: true
  use_rmsnorm: true
  wo_shift: false
  in_chans: 32

# training parameters
train:
  max_steps: 80000
  # We use large batch training (1024) with adjusted learning rate and beta2 accordingly
  # this is inspired by AuraFlow and muP.
  global_batch_size: 1024
  global_seed: 0
  output_dir: 'output'
  exp_name: 'lightningdit_xl_vavae_f16d32'
  ckpt: null
  log_every: 100
  ckpt_every: 20000
optimizer:
  lr: 0.0002
  beta2: 0.95

# we use rectified flow for fast training.
transport:
  # We inherit these settings from SiT, no parameters are changed
  path_type: Linear
  prediction: velocity
  loss_weight: null
  sample_eps: null
  train_eps: null

  # Inspired by SD3 and our previous work FasterDiT
  # In small-scale experiments, we enable lognorm
  # In large-scale experiments, we disable lognorm at the mid of training
  use_lognorm: true
  # cosine loss is enabled at all times
  use_cosine_loss: true

sample:
  mode: ODE
  # here we mainly adopt 2 settings: 1. dopri5, 2. euler
  # dopri5 has adaptive step size, which is faster but has a slight performance drop
  sampling_method: euler
  atol: 0.000001
  rtol: 0.001
  reverse: false
  likelihood: false
  num_sampling_steps: 250
  cfg_scale: 8.0 # <---- cfg scale, for 800 epoch performance with FID=1.35 cfg_scale=6.7
                 #       for 64 epoch performance with FID=2.11 cfg_scale=10.0
                 #       you may find we use a large cfg_scale, this is because of 2 reasons:
                 #       (1) we find a high-dimensional latent space requires a large cfg_scale to get good performance than f8d4 SD-VAE
                 #       (2) we enable cfg interval, which reduces the negative effects of large cfg on high-noise parts. This means larger cfg can be utilized
  per_proc_batch_size: 4
  fid_num: 50000

  # cfg interval, it is inspired by <https://arxiv.org/abs/2404.07724>
  cfg_interval_start: 0.11
  # timestep shift, it is inspired by FLUX. please refer to transport/integrators.py ode function for details.
  timestep_shift: 0.3